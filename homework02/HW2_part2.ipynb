{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Z8kx4R9B5GmQ"
   },
   "source": [
    "# Homework 2, *part 2* (60 points)\n",
    "\n",
    "In this assignment you will build a convolutional neural net (CNN) to solve Tiny ImageNet image classification. Try to achieve as high accuracy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nZKOkVX5GmS"
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "* This file,\n",
    "* a \"checkpoint file\" from `torch.save(model.state_dict(), ...)` that contains model's weights (which a TA should be able to load to verify your accuracy).\n",
    "\n",
    "## Grading\n",
    "\n",
    "* 9 points for reproducible training code and a filled report below.\n",
    "* 12 points for building a network that gets above 20% accuracy.\n",
    "* 6.5 points for beating each of these milestones on the private **test** set:\n",
    "  * 25.0%\n",
    "  * 30.0%\n",
    "  * 32.5%\n",
    "  * 35.0%\n",
    "  * 37.5%\n",
    "  * 40.0%\n",
    "  \n",
    "*Private test set* means that you won't be able to evaluate your model on it. Rather, after you submit code and checkpoint, we will load your model and evaluate it on that test set ourselves (so please make sure it's easy for TAs to do!), reporting your accuracy in a comment to the grade.\n",
    "    \n",
    "## Restrictions\n",
    "\n",
    "* Don't use pretrained networks.\n",
    "\n",
    "## Tips\n",
    "\n",
    "* One change at a time: never test several new things at once.\n",
    "* Google a lot.\n",
    "* Use GPU.\n",
    "* Use regularization: L2, batch normalization, dropout, data augmentation.\n",
    "* Use Tensorboard ([non-Colab](https://github.com/lanpa/tensorboardX) or [Colab](https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6)) or a similar interactive tool for viewing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkYZC_MZZQ-6"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUSinJr85GmT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2929,
     "status": "ok",
     "timestamp": 1555507038008,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "0CPXa6gjou2-",
    "outputId": "504d5c52-c9db-450b-9c42-d99919af7506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json    model35.pt  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mtiny-imagenet-200\u001b[0m/     tiny_imagenet.py\n",
      "model31.pt  model36.pt  \u001b[01;34msample_data\u001b[0m/  tiny-imagenet-200.zip\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22405,
     "status": "ok",
     "timestamp": 1555482293007,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "Nng5Kusx5uAl",
    "outputId": "eb9874be-d7d1-4716-e93b-a8362c42493b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f13704e1-e174-4d6a-b2b3-a88f782c53aa\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f13704e1-e174-4d6a-b2b3-a88f782c53aa\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tiny_imagenet.py to tiny_imagenet.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tiny_imagenet.py': b'import os\\nfrom urllib.request import urlretrieve\\n\\ndef download(path, url=\\'http://cs231n.stanford.edu/tiny-imagenet-200.zip\\'):\\n    dataset_name = \\'tiny-imagenet-200\\'\\n\\n    if os.path.exists(os.path.join(path, dataset_name, \"val\", \"n01443537\")):\\n        print(\"%s already exists, not downloading\" % os.path.join(path, dataset_name))\\n        return\\n    else:\\n        print(\"Dataset not exists or is broken, downloading it\")\\n    urlretrieve(url, os.path.join(path, dataset_name + \".zip\"))\\n    \\n    import zipfile\\n    with zipfile.ZipFile(os.path.join(path, dataset_name + \".zip\"), \\'r\\') as archive:\\n        archive.extractall()\\n\\n    # move validation images to subfolders by class\\n    val_root = os.path.join(path, dataset_name, \"val\")\\n    with open(os.path.join(val_root, \"val_annotations.txt\"), \\'r\\') as f:\\n        for image_filename, class_name, _, _, _, _ in map(str.split, f):\\n            class_path = os.path.join(val_root, class_name)\\n            os.makedirs(class_path, exist_ok=True)\\n            os.rename(\\n                os.path.join(val_root, \"images\", image_filename),\\n                os.path.join(class_path, image_filename))\\n\\n    os.rmdir(os.path.join(val_root, \"images\"))\\n    os.remove(os.path.join(val_root, \"val_annotations.txt\"))\\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25216,
     "status": "ok",
     "timestamp": 1555482321291,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "zE8Mi0BT5GmX",
    "outputId": "96fcbf29-7629-47d9-b2a1-19763c1789e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not exists or is broken, downloading it\n"
     ]
    }
   ],
   "source": [
    "import tiny_imagenet\n",
    "tiny_imagenet.download(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2012,
     "status": "ok",
     "timestamp": 1555482330182,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "VhFf-vaI5545",
    "outputId": "0096ae87-76a8-4073-c0e4-cd8cad81f583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json      \u001b[0m\u001b[01;34msample_data\u001b[0m/        tiny-imagenet-200.zip\n",
      "\u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mtiny-imagenet-200\u001b[0m/  tiny_imagenet.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3otIfg9_5Gmc"
   },
   "source": [
    "Training and validation images are now in `tiny-imagenet-200/train` and `tiny-imagenet-200/val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnnx4bMT5Gmd"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hLL9UaWHzOn"
   },
   "outputs": [],
   "source": [
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment_train = transforms.Compose([\n",
    "   # decribe transformation here\n",
    "   transforms.RandomRotation((-30,30)),\n",
    "   transforms.RandomHorizontalFlip(),\n",
    "   transforms.RandomCrop(50),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(means, stds),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxL3h26xiQzl"
   },
   "outputs": [],
   "source": [
    "?transforms.RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UToTfL_z5Gmg"
   },
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform_augment_train)\n",
    "# test_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/val', transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [90000, 10000])\n",
    "# test_dataset, val_dataset = torch.utils.data.random_split(val_dataset, [10000, 10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_lglh8N5Gmi"
   },
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjN1M3z05Gmk"
   },
   "source": [
    "# Building a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OlhudYs5Gml"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgUdMO8WeI1E"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_sg6xwc3ivo"
   },
   "outputs": [],
   "source": [
    "# Inception v3 inspired\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.layer = nn.Sequential()\n",
    "        self.layer.add_module(\"conv\", nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False))\n",
    "        self.layer.add_module(\"bn\", nn.BatchNorm2d(out_planes,\n",
    "                                                   eps=0.001, # value found in tensorflow\n",
    "                                                   momentum=0.1, # default pytorch value\n",
    "                                                   affine=True))\n",
    "        self.layer.add_module(\"relu\", nn.ReLU(inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "      \n",
    "      \n",
    "      \n",
    "\n",
    "class MyConvNet (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        self.convolutions = nn.Sequential()\n",
    "        # N x 3 x 64 x 64\n",
    "        self.convolutions.add_module(\"conv0\", BasicConv2d(3, 32, kernel_size=3, stride=2))\n",
    "        # N x 32 x 31 x 31\n",
    "        self.convolutions.add_module(\"conv1\", BasicConv2d(32, 32, kernel_size=3))\n",
    "        # N x 32 x 29 x 29\n",
    "        self.convolutions.add_module(\"conv2\", BasicConv2d(32, 64, kernel_size=3, padding=1))\n",
    "        # N x 64 x 29 x 29\n",
    "        self.convolutions.add_module(\"drop6\", nn.Dropout())\n",
    "        self.convolutions.add_module(\"conv3\", BasicConv2d(64, 80, kernel_size=1, stride=2))\n",
    "        # N x 80 x 29 x 29\n",
    "        self.convolutions.add_module(\"conv4\", BasicConv2d(80, 192, kernel_size=3))\n",
    "        # N x 192 x 27 x 27\n",
    "\n",
    "        self.convolutions.add_module(\"pool\", nn.MaxPool2d(2))\n",
    "        \n",
    "        self.convolutions.add_module(\"conv5\", BasicConv2d(192, 400, kernel_size=3, padding=1))\n",
    "        self.convolutions.add_module(\"conv6\", BasicConv2d(400, 150, kernel_size=3, padding=1))\n",
    "        self.convolutions.add_module(\"conv7\", BasicConv2d(150, 64, kernel_size=3, padding=1))\n",
    "        self.convolutions.add_module(\"drop9\", nn.Dropout())\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module(\"flatten\", Flatten())\n",
    "        self.classifier.add_module(\"lin0\", nn.Linear(1024, 200))\n",
    "#         self.classifier.add_module(\"lin0_bn\", nn.BatchNorm1d(1000))\n",
    "#         self.classifier.add_module(\"lin0_relu\", nn.ReLU())\n",
    "#         self.classifier.add_module(\"lin1\", nn.Linear(1000, 200))\n",
    "        \n",
    "        \n",
    "#         self.convolutions.add_module(\"conv5\", InceptionA(192, pool_features=32))\n",
    "#         # N x 256 x 27 x 27\n",
    "#         self.convolutions.add_module(\"conv6\", InceptionB(192))\n",
    "        # N x 736 x 13 x 13\n",
    "#         self.convolutions.add_module(\"conv7\", InceptionC(736, channels_7x7=64))\n",
    "#         # N x 768 x 13 x 13\n",
    "#         self.convolutions.add_module(\"conv8\", InceptionD(672))\n",
    "        # N x 1280 x 6 x 6\n",
    "#         self.convolutions.add_module(\"conv9\", InceptionE(1280))\n",
    "#         # N x 2048 x 6 x 6\n",
    "       \n",
    "        \n",
    "        # adaptive_avg_pool2d\n",
    "        # N x 2048 x 1 x 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "#         x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "#         Flatten(),\n",
    "#         x = nn.functional.dropout(x, p=0.3)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "# #         # N x 1184\n",
    "        \n",
    "#         fc = nn.Linear(1184, 200).cuda()\n",
    "#         x = fc(x)\n",
    "# #         # N x 200\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "      \n",
    "model = MyConvNet()\n",
    "model = model.cuda()\n",
    "# model(torch.zeros(1,3,64,64).cuda()).shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eKlcWCQ5Gmr"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    "\n",
    "  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(64),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(128),\n",
    "  #nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    " \n",
    "  nn.Conv2d(in_channels=128, out_channels=250, kernel_size=3, padding=(0, 4)),\n",
    "  nn.BatchNorm2d(250),\n",
    "  nn.ReLU(),\n",
    "\n",
    "    \n",
    "  nn.Conv2d(in_channels=250, out_channels=250, kernel_size=3, padding=(4, 0)),\n",
    "  nn.BatchNorm2d(250),\n",
    "  nn.ReLU(),\n",
    "    \n",
    "#   nn.Conv2d(in_channels=250, out_channels=400, kernel_size=3, padding=1),\n",
    "#   nn.BatchNorm2d(400),\n",
    "#   nn.ReLU(),\n",
    "    \n",
    "#   nn.Conv2d(in_channels=400, out_channels=400, kernel_size=3, padding=1),\n",
    "#   nn.BatchNorm2d(400),\n",
    "#   nn.ReLU(),\n",
    "#   nn.Dropout(p=0.3), \n",
    "\n",
    "#   nn.Conv2d(in_channels=400, out_channels=250, kernel_size=3, padding=1),\n",
    "#   nn.BatchNorm2d(250),\n",
    "#   nn.ReLU(),\n",
    "\n",
    "    \n",
    "  nn.Conv2d(in_channels=250, out_channels=128, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(128),\n",
    "  nn.ReLU(), \n",
    "    \n",
    "  nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3),\n",
    "  nn.BatchNorm2d(64),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    "\n",
    "  Flatten(),\n",
    "  nn.Linear(12544, 1024),\n",
    "  nn.BatchNorm1d(1024),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),\n",
    "  nn.Linear(1024, 200)\n",
    ")\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "# model(torch.zeros(1,3,64,64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sj8Nk5fQdNk1"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(512, 200)\n",
    "model = model.cuda()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUGFy7mx5Gmw"
   },
   "outputs": [],
   "source": [
    "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# opt = torch.optim.RMSprop(model.parameters())\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNh5PEomoeOQ"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56889,
     "status": "ok",
     "timestamp": 1555517939522,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "bn7L74oe5Gmz",
    "outputId": "972076eb-c531-4cf5-e019-512c09a02ec4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 56.164s\n",
      "  training loss (in-iteration): \t1.120944\n",
      "  validation accuracy: \t\t\t40.57 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 # total amount of full passes over training data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in train_batch_gen:\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))\n",
    "    \n",
    "    \n",
    "#     clear_output(True)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(train_loss)\n",
    "#     plt.title('train_loss')\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(val_accuracy)\n",
    "#     plt.title('val_acc')\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajE-9BFI5Gm1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLjS2l-75Gm3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vogQacO65Gm5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvvHrmBk5Gm7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Wos-xZJ5GnA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clYFygUQ5GnC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6peqEiCt5GnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrKOuykg5GnH"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),  'model41.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3xzcV7z5GnJ"
   },
   "outputs": [],
   "source": [
    "model_loaded = torch.load('model36.pt', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2510,
     "status": "ok",
     "timestamp": 1555517126912,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "YPee6kdIK1lv",
    "outputId": "eea86480-f976-4368-b092-55a4759f6a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json    model35.pt  model40.pt    \u001b[0m\u001b[01;34msample_data\u001b[0m/        tiny-imagenet-200.zip\n",
      "model31.pt  model36.pt  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mtiny-imagenet-200\u001b[0m/  tiny_imagenet.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLRH_Z2xK1oX"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('model41.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SuT8sAxK1qN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MY9zkIiK1sL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHo8ynb55GnN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NE2RJYb5GnP"
   },
   "source": [
    "When everything is done, please compute accuracy on the validation set and report it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1555517674855,
     "user": {
      "displayName": "Evgeny Tsykunov",
      "photoUrl": "https://lh4.googleusercontent.com/-i1tzoBK_PgE/AAAAAAAAAAI/AAAAAAAAMOc/OcpUB27Wy2I/s64/photo.jpg",
      "userId": "06101548677895852703"
     },
     "user_tz": -180
    },
    "id": "b19l60M35GnP",
    "outputId": "5644c40c-41e6-4a09-d63a-3be28d60e366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  val accuracy:\t\t40.93 %\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "val_batch_acc = []\n",
    "for X_batch, y_batch in val_batch_gen:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data\n",
    "    val_batch_acc.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "\n",
    "val_accuracy_final = np.mean(val_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  val accuracy:\\t\\t{:.2f} %\".format(\n",
    "    val_accuracy_final * 100))\n",
    "\n",
    "# if test_accuracy * 100 > 70:\n",
    "#     print(\"U'r freakin' amazin'!\")\n",
    "# elif test_accuracy * 100 > 50:\n",
    "#     print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "# elif test_accuracy * 100 > 40:\n",
    "#     print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "# elif test_accuracy * 100 > 30:\n",
    "#     print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "# elif test_accuracy * 100 > 20:\n",
    "#     print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "# else:\n",
    "#     print(\"We need more magic! Follow instructons below\")\n",
    "\n",
    "# val_accuracy = # Your code here\n",
    "# print(\"Validation accuracy: %.2f%%\" % (val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gOc3UCI5GnQ"
   },
   "source": [
    "# Report\n",
    "\n",
    "Below, please mention\n",
    "\n",
    "* a brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method (batch size, optimization algorithm, ...) and why?\n",
    "* Any regularization and other techniques applied and their effects;\n",
    "\n",
    "The reference format is:\n",
    "\n",
    "*\"I have analyzed these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGKQxSlp5GnR"
   },
   "source": [
    "# Regularization\n",
    "\n",
    "The first thing I did was to take care of reularization.\n",
    "\n",
    "I applied data augmentation with\n",
    "transform_augment_train = transforms.Compose([\n",
    "   transforms.RandomRotation((-30,30)),\n",
    "   transforms.RandomHorizontalFlip(),\n",
    "   transforms.RandomCrop(50),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "Before creating the net architecture i kept in mind to insert several Dropouts for the convolutional part and one before every dence layer. I also gona to insert batchnorm after every convolition and after every dence layer (except the last one).\n",
    "\n",
    "# First simple seld-developed convnet\n",
    "\n",
    "Right from the beginning I started to build the typical arcitecture: conv -> batchnorm -> nonlinear.\n",
    "The number of pulling has to provede the propor receptive field.\n",
    "The most complicated thing here is to figure out the number of channels, number of layers, and other paramaters.\n",
    "I ddecided to start with the simplest architecture. The batch_size was set to 500 (slightly big for better regularization).\n",
    "For the Adam optimizer the LR was set to 0.01, which is pretty fast.\n",
    "_________________________________________________________________________\n",
    "StartUp architecture:\n",
    "\n",
    "Very simple, just to make sure it works somehow. Consist of tree convolutions with three  \n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "  nn.BatchNorm2d(64),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "  nn.BatchNorm2d(128),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n",
    "  nn.BatchNorm2d(256),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  Flatten(),\n",
    "  nn.Dropout(0,2),\n",
    "  nn.Linear(256, 1024),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(0,2),\n",
    "  nn.Linear(1024, 200)\n",
    ")\n",
    "\n",
    "Receptive fiels covers almost all image.\n",
    "\n",
    "Epoch 52 of 100 took 42.022s\n",
    "  training loss (in-iteration): \t2.922985\n",
    "  validation accuracy: \t\t\t27.90 %\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "# More advanced seld-developed convnet\n",
    "\n",
    "The next step was the obvious - to make the net more fat.\n",
    "\n",
    "More advanced network:\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    "\n",
    "  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(64),\n",
    "  nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(128),\n",
    "  #nn.MaxPool2d(2),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    " \n",
    "  nn.Conv2d(in_channels=128, out_channels=250, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(250),\n",
    "  nn.ReLU(),\n",
    "    \n",
    "  nn.Conv2d(in_channels=250, out_channels=250, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(250),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    "\n",
    "  nn.Conv2d(in_channels=250, out_channels=128, kernel_size=3, padding=1),\n",
    "  nn.BatchNorm2d(128),\n",
    "  nn.ReLU(), \n",
    "    \n",
    "  nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3),\n",
    "  nn.BatchNorm2d(64),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),  \n",
    "\n",
    "  Flatten(),\n",
    "  nn.Linear(6400, 512),\n",
    "  nn.BatchNorm1d(512),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(p=0.3),\n",
    "  nn.Linear(512, 200)\n",
    ")\n",
    "\n",
    "\n",
    "Epoch 58 of 100 took 50.212s\n",
    "  training loss (in-iteration): \t2.273430\n",
    "  validation accuracy: \t\t\t31.19 %\n",
    "  \n",
    "  \n",
    "Decreasing the batch size down to 150 and Change of LR to default (0.001) for adam increased the val accuracy to 35%:\n",
    "  \n",
    "  Epoch 41 of 100 took 51.407s\n",
    "  training loss (in-iteration): \t2.322155\n",
    "  validation accuracy: \t\t\t35.41 %\n",
    "  \n",
    "  \n",
    "________________________________________________________________________\n",
    "# Adaptation of Inception v3\n",
    "\n",
    "I tried to build more advanced network out of Inception v3 blocks:\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.layer = nn.Sequential()\n",
    "        self.layer.add_module(\"conv\", nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False))\n",
    "        self.layer.add_module(\"bn\", nn.BatchNorm2d(out_planes,\n",
    "                                                   eps=0.001, # value found in tensorflow\n",
    "                                                   momentum=0.1, # default pytorch value\n",
    "                                                   affine=True))\n",
    "        self.layer.add_module(\"relu\", nn.ReLU(inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "      \n",
    "class MyConvNet (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        self.convolutions = nn.Sequential()\n",
    "        # N x 3 x 64 x 64\n",
    "        self.convolutions.add_module(\"conv0\", BasicConv2d(3, 32, kernel_size=3, stride=2))\n",
    "        # N x 32 x 31 x 31\n",
    "        self.convolutions.add_module(\"conv1\", BasicConv2d(32, 32, kernel_size=3))\n",
    "        # N x 32 x 29 x 29\n",
    "        self.convolutions.add_module(\"conv2\", BasicConv2d(32, 64, kernel_size=3, padding=1))\n",
    "        # N x 64 x 29 x 29\n",
    "        self.convolutions.add_module(\"drop6\", nn.Dropout())\n",
    "        self.convolutions.add_module(\"conv3\", BasicConv2d(64, 80, kernel_size=1, stride=2))\n",
    "        # N x 80 x 29 x 29\n",
    "        self.convolutions.add_module(\"conv4\", BasicConv2d(80, 192, kernel_size=3))\n",
    "        # N x 192 x 27 x 27\n",
    "\n",
    "         self.convolutions.add_module(\"conv5\", InceptionA(192, pool_features=32))\n",
    "         # N x 256 x 27 x 27\n",
    "         self.convolutions.add_module(\"conv6\", InceptionB(192))\n",
    "        # N x 736 x 13 x 13\n",
    "         self.convolutions.add_module(\"conv7\", InceptionC(736, channels_7x7=64))\n",
    "         # N x 768 x 13 x 13\n",
    "         self.convolutions.add_module(\"conv8\", InceptionD(672))\n",
    "        # N x 1280 x 6 x 6\n",
    "         self.convolutions.add_module(\"conv9\", InceptionE(1280))\n",
    "         # N x 2048 x 6 x 6\n",
    "       \n",
    "        \n",
    "        # adaptive_avg_pool2d\n",
    "        # N x 2048 x 1 x 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "       \n",
    "         x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "\n",
    "         x = nn.functional.dropout(x, p=0.3)\n",
    "         x = x.view(x.size(0), -1)\n",
    "        # N x 1184\n",
    "        \n",
    "         fc = nn.Linear(1184, 200).cuda()\n",
    "        x = fc(x)\n",
    "        # N x 200\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "But I did not succeed with that. It was too hard for me to do it without mistakes - learning rate was too slow..\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "# Usage of RESNET\n",
    "\n",
    "After some straggles with self-builded network, I decided that it should be relly easies to use a well developed architecture. ResNet is a good candidate for reusing. It was not prohibited to use a net with no pretrained parameters. \n",
    "\n",
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(512, 200) # to modify the last layer for our number of classes.\n",
    "\n",
    "ResNet out of the box was able to give me 35-36% of validation accuracy with default parameters for Adam optimizer.\n",
    "To get better result, after getting 35% of val accuracy,  I reduced the learning rate down to 0.0001, then 0.00001 (doing small steps to get deeper in gradient descend) to train the net up to 40.93% - my best score.\n",
    "\n",
    "L2 regularization is not effective in Adam. Source: https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0. So I did't use it here.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_part2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
